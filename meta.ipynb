{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-21 17:47:09,532] A new study created in memory with name: no-name-4cffa88c-6208-485e-8ea4-9af2dc811297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Starting improved preprocessing...\n",
      "\n",
      "--- Stage 1: Optimizing Hyperparameters with Optuna ---\n",
      "\n",
      "Optimizing LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-21 17:47:12,703] Trial 0 finished with value: 0.13014499497422152 and parameters: {'learning_rate': 0.009522805662926726, 'lambda_l1': 1.8659325751746594e-08, 'lambda_l2': 0.06737190377357355, 'num_leaves': 100, 'feature_fraction': 0.9587287102354394, 'bagging_fraction': 0.6128349067020851, 'bagging_freq': 6, 'min_child_samples': 25}. Best is trial 0 with value: 0.13014499497422152.\n",
      "[I 2025-09-21 17:47:15,587] Trial 1 finished with value: 0.13019188623233222 and parameters: {'learning_rate': 0.04110241235912489, 'lambda_l1': 2.6827373081626433, 'lambda_l2': 2.435178881979817, 'num_leaves': 78, 'feature_fraction': 0.8629297889358947, 'bagging_fraction': 0.9318669982951457, 'bagging_freq': 3, 'min_child_samples': 22}. Best is trial 0 with value: 0.13014499497422152.\n",
      "[I 2025-09-21 17:47:17,288] Trial 2 finished with value: 0.13224016505058742 and parameters: {'learning_rate': 0.030435983684446943, 'lambda_l1': 3.2160886691702995e-07, 'lambda_l2': 7.209411087602624e-07, 'num_leaves': 41, 'feature_fraction': 0.5755077515974929, 'bagging_fraction': 0.6553794326230454, 'bagging_freq': 4, 'min_child_samples': 43}. Best is trial 0 with value: 0.13014499497422152.\n",
      "[I 2025-09-21 17:47:19,270] Trial 3 finished with value: 0.1327225776409695 and parameters: {'learning_rate': 0.020377088709137202, 'lambda_l1': 0.0002029237149962472, 'lambda_l2': 3.168213503383533e-05, 'num_leaves': 38, 'feature_fraction': 0.7803414660676471, 'bagging_fraction': 0.9108380312772294, 'bagging_freq': 1, 'min_child_samples': 63}. Best is trial 0 with value: 0.13014499497422152.\n",
      "[I 2025-09-21 17:47:21,415] Trial 4 finished with value: 0.12910398988516442 and parameters: {'learning_rate': 0.023744593588471617, 'lambda_l1': 1.278678602511232e-08, 'lambda_l2': 1.4441104591002431e-08, 'num_leaves': 20, 'feature_fraction': 0.629981560974275, 'bagging_fraction': 0.9367501656442722, 'bagging_freq': 3, 'min_child_samples': 23}. Best is trial 4 with value: 0.12910398988516442.\n",
      "[I 2025-09-21 17:47:22,240] Trial 5 finished with value: 0.14692696683128745 and parameters: {'learning_rate': 0.039270321694077584, 'lambda_l1': 0.030509895248783708, 'lambda_l2': 1.0087680648510394e-08, 'num_leaves': 46, 'feature_fraction': 0.4250059057971123, 'bagging_fraction': 0.4489099888913026, 'bagging_freq': 4, 'min_child_samples': 89}. Best is trial 4 with value: 0.12910398988516442.\n",
      "[I 2025-09-21 17:47:23,887] Trial 6 finished with value: 0.14166196991812616 and parameters: {'learning_rate': 0.00854808292729351, 'lambda_l1': 1.6486320022911163, 'lambda_l2': 1.9302653309046953e-06, 'num_leaves': 74, 'feature_fraction': 0.6784696315408606, 'bagging_fraction': 0.7061999335290108, 'bagging_freq': 3, 'min_child_samples': 71}. Best is trial 4 with value: 0.12910398988516442.\n",
      "[I 2025-09-21 17:47:24,864] Trial 7 finished with value: 0.15461930475823202 and parameters: {'learning_rate': 0.026456718062800987, 'lambda_l1': 3.6121237839929294e-06, 'lambda_l2': 9.641720632111616e-06, 'num_leaves': 57, 'feature_fraction': 0.5349812638494177, 'bagging_fraction': 0.4232575826212816, 'bagging_freq': 3, 'min_child_samples': 95}. Best is trial 4 with value: 0.12910398988516442.\n",
      "[I 2025-09-21 17:47:25,390] Trial 8 finished with value: 0.14095659270214664 and parameters: {'learning_rate': 0.03094497386114209, 'lambda_l1': 1.815151390532043e-07, 'lambda_l2': 0.03654552254730618, 'num_leaves': 77, 'feature_fraction': 0.7018662649578769, 'bagging_fraction': 0.5119639578763863, 'bagging_freq': 6, 'min_child_samples': 51}. Best is trial 4 with value: 0.12910398988516442.\n",
      "[I 2025-09-21 17:47:27,946] Trial 9 finished with value: 0.13724893121177573 and parameters: {'learning_rate': 0.006003916945739329, 'lambda_l1': 1.1379373976924663, 'lambda_l2': 1.2291017214063818e-05, 'num_leaves': 42, 'feature_fraction': 0.4126524631674734, 'bagging_fraction': 0.4064569931703733, 'bagging_freq': 1, 'min_child_samples': 32}. Best is trial 4 with value: 0.12910398988516442.\n",
      "[I 2025-09-21 17:47:30,383] Trial 10 finished with value: 0.13066076636207838 and parameters: {'learning_rate': 0.018954591296318456, 'lambda_l1': 0.00012709048057293074, 'lambda_l2': 1.9273626770588665e-08, 'num_leaves': 21, 'feature_fraction': 0.6005254129030965, 'bagging_fraction': 0.9979214749552887, 'bagging_freq': 7, 'min_child_samples': 39}. Best is trial 4 with value: 0.12910398988516442.\n",
      "[I 2025-09-21 17:47:36,496] Trial 11 finished with value: 0.12587133672052644 and parameters: {'learning_rate': 0.015339616766541586, 'lambda_l1': 1.5397817643907645e-08, 'lambda_l2': 0.0025838103519889204, 'num_leaves': 100, 'feature_fraction': 0.9816939570981422, 'bagging_fraction': 0.737138948432558, 'bagging_freq': 6, 'min_child_samples': 5}. Best is trial 11 with value: 0.12587133672052644.\n",
      "[I 2025-09-21 17:47:42,912] Trial 12 finished with value: 0.12422324463461326 and parameters: {'learning_rate': 0.016411572434220194, 'lambda_l1': 2.4732680704759326e-08, 'lambda_l2': 0.0008776168263582956, 'num_leaves': 95, 'feature_fraction': 0.9592689641659704, 'bagging_fraction': 0.7939686402982552, 'bagging_freq': 5, 'min_child_samples': 6}. Best is trial 12 with value: 0.12422324463461326.\n",
      "[I 2025-09-21 17:47:48,541] Trial 13 finished with value: 0.1259588874668243 and parameters: {'learning_rate': 0.01540146253444025, 'lambda_l1': 2.9738444000698416e-06, 'lambda_l2': 0.0015323847173761593, 'num_leaves': 99, 'feature_fraction': 0.9734105968293594, 'bagging_fraction': 0.7883326695862571, 'bagging_freq': 6, 'min_child_samples': 8}. Best is trial 12 with value: 0.12422324463461326.\n",
      "[I 2025-09-21 17:47:51,378] Trial 14 finished with value: 0.1275522798784907 and parameters: {'learning_rate': 0.04959244726240321, 'lambda_l1': 1.2005502175942744e-05, 'lambda_l2': 0.001208913498475253, 'num_leaves': 89, 'feature_fraction': 0.8797807554735675, 'bagging_fraction': 0.790032292780189, 'bagging_freq': 5, 'min_child_samples': 7}. Best is trial 12 with value: 0.12422324463461326.\n",
      "[I 2025-09-21 17:48:02,886] Trial 15 finished with value: 0.12313052146169931 and parameters: {'learning_rate': 0.01363970940339849, 'lambda_l1': 0.022459818133879955, 'lambda_l2': 0.04381030772057137, 'num_leaves': 88, 'feature_fraction': 0.8744013634331522, 'bagging_fraction': 0.7883799414516603, 'bagging_freq': 7, 'min_child_samples': 5}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:06,810] Trial 16 finished with value: 0.12751232114394095 and parameters: {'learning_rate': 0.013164234689582818, 'lambda_l1': 0.048458971036062226, 'lambda_l2': 4.036556243774678, 'num_leaves': 88, 'feature_fraction': 0.8638272210865325, 'bagging_fraction': 0.8384207509903653, 'bagging_freq': 7, 'min_child_samples': 15}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:08,670] Trial 17 finished with value: 0.1384577160484899 and parameters: {'learning_rate': 0.020122969632849666, 'lambda_l1': 0.004832916588484448, 'lambda_l2': 0.03819018156706415, 'num_leaves': 65, 'feature_fraction': 0.7772043235370034, 'bagging_fraction': 0.853856803007004, 'bagging_freq': 5, 'min_child_samples': 77}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:13,263] Trial 18 finished with value: 0.12799658951012297 and parameters: {'learning_rate': 0.011213566027788772, 'lambda_l1': 0.0026608026127720595, 'lambda_l2': 0.22489490171398963, 'num_leaves': 88, 'feature_fraction': 0.7851438990557997, 'bagging_fraction': 0.5838159965438711, 'bagging_freq': 5, 'min_child_samples': 16}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:15,101] Trial 19 finished with value: 0.13056094936305354 and parameters: {'learning_rate': 0.024005399016361617, 'lambda_l1': 0.10366256835055747, 'lambda_l2': 0.00024040292344844954, 'num_leaves': 68, 'feature_fraction': 0.9113318956643821, 'bagging_fraction': 0.7741031431773361, 'bagging_freq': 7, 'min_child_samples': 31}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:17,223] Trial 20 finished with value: 0.13245319988265875 and parameters: {'learning_rate': 0.03457951241709183, 'lambda_l1': 0.001212896694926754, 'lambda_l2': 0.008401967316223806, 'num_leaves': 84, 'feature_fraction': 0.8110864046636278, 'bagging_fraction': 0.8605741587565927, 'bagging_freq': 2, 'min_child_samples': 56}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:24,456] Trial 21 finished with value: 0.12602500428064928 and parameters: {'learning_rate': 0.015853004606562218, 'lambda_l1': 1.485036942090293e-07, 'lambda_l2': 0.0001347450803925106, 'num_leaves': 93, 'feature_fraction': 0.9917894664640226, 'bagging_fraction': 0.7211281844127028, 'bagging_freq': 6, 'min_child_samples': 5}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:29,999] Trial 22 finished with value: 0.12421682730940169 and parameters: {'learning_rate': 0.01547486810309524, 'lambda_l1': 7.392560132827666e-08, 'lambda_l2': 0.0046892092409971334, 'num_leaves': 100, 'feature_fraction': 0.9309629447033835, 'bagging_fraction': 0.7313350648102503, 'bagging_freq': 5, 'min_child_samples': 13}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:37,132] Trial 23 finished with value: 0.12783821520438216 and parameters: {'learning_rate': 0.005132212141160013, 'lambda_l1': 3.0365892738118176e-05, 'lambda_l2': 0.22126328533317755, 'num_leaves': 95, 'feature_fraction': 0.9190937297633797, 'bagging_fraction': 0.6593417252771363, 'bagging_freq': 5, 'min_child_samples': 15}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:44,337] Trial 24 finished with value: 0.12679836955347934 and parameters: {'learning_rate': 0.017137077363377218, 'lambda_l1': 6.951109558043654e-07, 'lambda_l2': 0.00811397371654632, 'num_leaves': 82, 'feature_fraction': 0.9210232186595911, 'bagging_fraction': 0.7499064282849167, 'bagging_freq': 4, 'min_child_samples': 15}. Best is trial 15 with value: 0.12313052146169931.\n",
      "[I 2025-09-21 17:48:44,341] A new study created in memory with name: no-name-0dac3230-6fdb-43e5-9e2b-5594b3868d61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM Params Found: {'learning_rate': 0.01363970940339849, 'lambda_l1': 0.022459818133879955, 'lambda_l2': 0.04381030772057137, 'num_leaves': 88, 'feature_fraction': 0.8744013634331522, 'bagging_fraction': 0.7883799414516603, 'bagging_freq': 7, 'min_child_samples': 5, 'n_estimators': 2000, 'verbose': -1, 'n_jobs': -1, 'seed': 42}\n",
      "\n",
      "Optimizing XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-21 17:48:46,854] Trial 0 finished with value: 0.13107895432995897 and parameters: {'n_estimators': 1061, 'learning_rate': 0.02724976352802264, 'lambda': 7.911429114896773e-07, 'alpha': 0.02387269325977189, 'max_depth': 8, 'subsample': 0.6333662593480134, 'colsample_bytree': 0.7851860309277903, 'min_child_weight': 8}. Best is trial 0 with value: 0.13107895432995897.\n",
      "[I 2025-09-21 17:48:48,026] Trial 1 finished with value: 0.1204870605818815 and parameters: {'n_estimators': 892, 'learning_rate': 0.019580361247763918, 'lambda': 0.0009358113631177011, 'alpha': 7.717397233417971e-07, 'max_depth': 4, 'subsample': 0.6825062865734908, 'colsample_bytree': 0.6394373067466941, 'min_child_weight': 5}. Best is trial 1 with value: 0.1204870605818815.\n",
      "[I 2025-09-21 17:48:49,701] Trial 2 finished with value: 0.12744174754129373 and parameters: {'n_estimators': 1226, 'learning_rate': 0.02983068472314354, 'lambda': 4.699907237914144, 'alpha': 1.45010861804612e-05, 'max_depth': 4, 'subsample': 0.7353664235990913, 'colsample_bytree': 0.7097927248352739, 'min_child_weight': 10}. Best is trial 1 with value: 0.1204870605818815.\n",
      "[I 2025-09-21 17:48:53,673] Trial 3 finished with value: 0.13054632550109138 and parameters: {'n_estimators': 1959, 'learning_rate': 0.023083542641523614, 'lambda': 0.00022670065698302412, 'alpha': 0.00017882862373155747, 'max_depth': 8, 'subsample': 0.9703045588644298, 'colsample_bytree': 0.5711167790622125, 'min_child_weight': 6}. Best is trial 1 with value: 0.1204870605818815.\n",
      "[I 2025-09-21 17:48:56,118] Trial 4 finished with value: 0.1262618924077887 and parameters: {'n_estimators': 1985, 'learning_rate': 0.0440287190477294, 'lambda': 0.0002762284685509646, 'alpha': 0.0005508795612511509, 'max_depth': 3, 'subsample': 0.8090564682744965, 'colsample_bytree': 0.6548502727345089, 'min_child_weight': 10}. Best is trial 1 with value: 0.1204870605818815.\n",
      "[I 2025-09-21 17:48:56,981] Trial 5 finished with value: 0.12741619834874512 and parameters: {'n_estimators': 714, 'learning_rate': 0.010436898774836037, 'lambda': 0.9860298713798298, 'alpha': 1.105536445267448e-06, 'max_depth': 3, 'subsample': 0.9935935774128797, 'colsample_bytree': 0.6768971931861816, 'min_child_weight': 8}. Best is trial 1 with value: 0.1204870605818815.\n",
      "[I 2025-09-21 17:49:01,264] Trial 6 finished with value: 0.12868082006196604 and parameters: {'n_estimators': 1993, 'learning_rate': 0.023809965285204765, 'lambda': 5.6950507199602794e-05, 'alpha': 1.2115703013922397e-06, 'max_depth': 9, 'subsample': 0.7623775367623016, 'colsample_bytree': 0.5011749308450709, 'min_child_weight': 3}. Best is trial 1 with value: 0.1204870605818815.\n",
      "[I 2025-09-21 17:49:04,759] Trial 7 finished with value: 0.13091687949142145 and parameters: {'n_estimators': 1906, 'learning_rate': 0.03631342046411403, 'lambda': 0.0004350624352308393, 'alpha': 0.023690275025368655, 'max_depth': 8, 'subsample': 0.8914813552468758, 'colsample_bytree': 0.9792197049861506, 'min_child_weight': 6}. Best is trial 1 with value: 0.1204870605818815.\n",
      "[I 2025-09-21 17:49:07,638] Trial 8 finished with value: 0.1276535885137633 and parameters: {'n_estimators': 1567, 'learning_rate': 0.013212666871441747, 'lambda': 0.00013170840475912294, 'alpha': 0.008585525715168036, 'max_depth': 6, 'subsample': 0.9077916233134651, 'colsample_bytree': 0.6917776831415912, 'min_child_weight': 4}. Best is trial 1 with value: 0.1204870605818815.\n",
      "[I 2025-09-21 17:49:08,933] Trial 9 finished with value: 0.11930540120548375 and parameters: {'n_estimators': 1033, 'learning_rate': 0.04233718891556497, 'lambda': 9.253921935324589e-07, 'alpha': 0.027041542303584815, 'max_depth': 3, 'subsample': 0.9029149951483717, 'colsample_bytree': 0.9569124626237102, 'min_child_weight': 2}. Best is trial 9 with value: 0.11930540120548375.\n",
      "[I 2025-09-21 17:49:09,694] Trial 10 finished with value: 0.14784654041548714 and parameters: {'n_estimators': 529, 'learning_rate': 0.04885343043627588, 'lambda': 4.038493878020342e-08, 'alpha': 7.113130983539551, 'max_depth': 6, 'subsample': 0.5118238314835211, 'colsample_bytree': 0.9746821918481037, 'min_child_weight': 1}. Best is trial 9 with value: 0.11930540120548375.\n",
      "[I 2025-09-21 17:49:11,154] Trial 11 finished with value: 0.12239966215930204 and parameters: {'n_estimators': 923, 'learning_rate': 0.03843274625837841, 'lambda': 0.028294850989545886, 'alpha': 1.1009720266429197e-08, 'max_depth': 4, 'subsample': 0.6244733931579393, 'colsample_bytree': 0.8219630296843914, 'min_child_weight': 1}. Best is trial 9 with value: 0.11930540120548375.\n",
      "[I 2025-09-21 17:49:13,045] Trial 12 finished with value: 0.13413215318772986 and parameters: {'n_estimators': 1410, 'learning_rate': 0.016686828255557774, 'lambda': 1.2816574478411978e-06, 'alpha': 4.519745698067791, 'max_depth': 5, 'subsample': 0.6309978586948528, 'colsample_bytree': 0.864759258062691, 'min_child_weight': 3}. Best is trial 9 with value: 0.11930540120548375.\n",
      "[I 2025-09-21 17:49:14,371] Trial 13 finished with value: 0.12590273230054067 and parameters: {'n_estimators': 889, 'learning_rate': 0.005507703178328053, 'lambda': 0.01896121244517864, 'alpha': 1.1632553351788023e-08, 'max_depth': 4, 'subsample': 0.8250862213641539, 'colsample_bytree': 0.920326562620926, 'min_child_weight': 4}. Best is trial 9 with value: 0.11930540120548375.\n",
      "[I 2025-09-21 17:49:15,656] Trial 14 finished with value: 0.12034183446379375 and parameters: {'n_estimators': 1142, 'learning_rate': 0.035966511449274464, 'lambda': 3.6377250787437848e-06, 'alpha': 0.4291797902433791, 'max_depth': 3, 'subsample': 0.7199127635821007, 'colsample_bytree': 0.6064613438171528, 'min_child_weight': 2}. Best is trial 9 with value: 0.11930540120548375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Params Found: {'n_estimators': 1033, 'learning_rate': 0.04233718891556497, 'lambda': 9.253921935324589e-07, 'alpha': 0.027041542303584815, 'max_depth': 3, 'subsample': 0.9029149951483717, 'colsample_bytree': 0.9569124626237102, 'min_child_weight': 2, 'n_jobs': -1, 'seed': 42}\n",
      "\n",
      "--- Starting Final Stacking Ensemble with Optimized Parameters---\n",
      "--- Fold 1/10 ---\n",
      "--- Fold 2/10 ---\n",
      "--- Fold 3/10 ---\n",
      "--- Fold 4/10 ---\n",
      "--- Fold 5/10 ---\n",
      "--- Fold 6/10 ---\n",
      "--- Fold 7/10 ---\n",
      "--- Fold 8/10 ---\n",
      "--- Fold 9/10 ---\n",
      "--- Fold 10/10 ---\n",
      "\n",
      "--- Training Meta-Model ---\n",
      "Overall Stacked CV RMSE: 0.11259\n",
      "\n",
      "--- Submission Complete ---\n",
      "Submission file 'submission.csv' created successfully.\n",
      "     Id      SalePrice\n",
      "0  1461  123049.885827\n",
      "1  1462  161340.383659\n",
      "2  1463  179879.051712\n",
      "3  1464  193523.175909\n",
      "4  1465  183813.606115\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import skew\n",
    "import optuna\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "def load_data():\n",
    "    \"\"\"Loads the training and testing datasets.\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError:\n",
    "        return None, None\n",
    "\n",
    "# --- 2. Preprocessing and Feature Engineering ---\n",
    "def preprocess(train_df, test_df):\n",
    "    \"\"\"Handles missing values, feature engineering, and normalization.\"\"\"\n",
    "    print(\"Starting improved preprocessing...\")\n",
    "    test_ids = test_df['Id']\n",
    "    train_df = train_df.drop('Id', axis=1)\n",
    "    test_df = test_df.drop('Id', axis=1)\n",
    "\n",
    "    train_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\n",
    "    train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n",
    "    y = train_df['SalePrice']\n",
    "    all_data = pd.concat((train_df.drop('SalePrice', axis=1), test_df))\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType',\n",
    "                'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n",
    "                'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "        all_data[col] = all_data[col].fillna('None')\n",
    "    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2',\n",
    "                'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "        all_data[col] = all_data[col].fillna(0)\n",
    "    for col in ('MSZoning', 'Utilities', 'Functional', 'Exterior1st', 'Exterior2nd',\n",
    "                'KitchenQual', 'SaleType', 'Electrical'):\n",
    "        all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "    all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # Feature Engineering\n",
    "    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "    all_data['Total_Bathrooms'] = (all_data['FullBath'] + 0.5 * all_data['HalfBath'] +\n",
    "                                   all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath'])\n",
    "    all_data['Total_Porch_SF'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] +\n",
    "                                  all_data['EnclosedPorch'] + all_data['ScreenPorch'] +\n",
    "                                  all_data['WoodDeckSF'])\n",
    "    all_data['YearBuilt_Age'] = all_data['YrSold'] - all_data['YearBuilt']\n",
    "    all_data['YearRemod_Age'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "    all_data['OverallQual_sq'] = all_data['OverallQual']**2\n",
    "    all_data['TotalSF_sq'] = all_data['TotalSF']**2\n",
    "    all_data['GrLivArea_sq'] = all_data['GrLivArea']**2\n",
    "    all_data['OverallQual_x_TotalSF'] = all_data['OverallQual'] * all_data['TotalSF']\n",
    "    all_data['GrLivArea_x_OverallQual'] = all_data['GrLivArea'] * all_data['OverallQual']\n",
    "\n",
    "    # Skewness Transform\n",
    "    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.75].index\n",
    "    all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "    \n",
    "    all_data = pd.get_dummies(all_data)\n",
    "\n",
    "    X = all_data[:len(y)]\n",
    "    X_test_competition = all_data[len(y):]\n",
    "\n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\n",
    "    X_test_competition = pd.DataFrame(scaler.transform(X_test_competition), index=X_test_competition.index, columns=X_test_competition.columns)\n",
    "    \n",
    "    return X, y, X_test_competition, test_ids\n",
    "\n",
    "# --- 3. Hyperparameter Optimization with Optuna ---\n",
    "def optimize_lgbm(trial, X, y):\n",
    "    params = {\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'verbose': -1, 'n_jobs': -1, 'seed': 42\n",
    "    }\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    preds = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "def optimize_xgb(trial, X, y):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000), # Let Optuna choose n_estimators\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'n_jobs': -1, 'seed': 42\n",
    "    }\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    # MODIFICATION: Removed early_stopping_rounds due to persistent environment error\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n",
    "    preds = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "# --- 4. Main Stacking and Submission Function ---\n",
    "def stacking_and_predict(X, y, X_test_competition, test_ids, lgb_params, xgb_params):\n",
    "    print(\"\\n--- Starting Final Stacking Ensemble with Optimized Parameters---\")\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    oof_preds_lgb = np.zeros(X.shape[0])\n",
    "    oof_preds_xgb = np.zeros(X.shape[0])\n",
    "    oof_preds_cat = np.zeros(X.shape[0])\n",
    "    test_preds_lgb = np.zeros(X_test_competition.shape[0])\n",
    "    test_preds_xgb = np.zeros(X_test_competition.shape[0])\n",
    "    test_preds_cat = np.zeros(X_test_competition.shape[0])\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "        print(f\"--- Fold {fold+1}/10 ---\")\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        oof_preds_lgb[val_index] = lgb_model.predict(X_val)\n",
    "        test_preds_lgb += lgb_model.predict(X_test_competition) / kf.n_splits\n",
    "\n",
    "        xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "        # MODIFICATION: Removed early_stopping_rounds\n",
    "        xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n",
    "        oof_preds_xgb[val_index] = xgb_model.predict(X_val)\n",
    "        test_preds_xgb += xgb_model.predict(X_test_competition) / kf.n_splits\n",
    "        \n",
    "        cat_model = CatBoostRegressor(iterations=2000, verbose=0, early_stopping_rounds=50, loss_function='RMSE', random_seed=42)\n",
    "        cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "        oof_preds_cat[val_index] = cat_model.predict(X_val)\n",
    "        test_preds_cat += cat_model.predict(X_test_competition) / kf.n_splits\n",
    "\n",
    "    X_meta_train = pd.DataFrame({'lgb': oof_preds_lgb, 'xgb': oof_preds_xgb, 'cat': oof_preds_cat})\n",
    "    X_meta_test = pd.DataFrame({'lgb': test_preds_lgb, 'xgb': test_preds_xgb, 'cat': test_preds_cat})\n",
    "\n",
    "    print(\"\\n--- Training Meta-Model ---\")\n",
    "    meta_model = ElasticNetCV(cv=5, random_state=42)\n",
    "    meta_model.fit(X_meta_train, y)\n",
    "    \n",
    "    oof_stacked_preds = meta_model.predict(X_meta_train)\n",
    "    stacked_rmse = np.sqrt(mean_squared_error(y, oof_stacked_preds))\n",
    "    print(f\"Overall Stacked CV RMSE: {stacked_rmse:.5f}\")\n",
    "\n",
    "    final_predictions_log = meta_model.predict(X_meta_test)\n",
    "    final_predictions = np.expm1(final_predictions_log)\n",
    "\n",
    "    submission = pd.DataFrame({'Id': test_ids, 'SalePrice': final_predictions})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(\"\\n--- Submission Complete ---\")\n",
    "    print(\"Submission file 'submission.csv' created successfully.\")\n",
    "    print(submission.head())\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    train_df, test_df = load_data()\n",
    "    if train_df is not None and test_df is not None:\n",
    "        X, y, X_test_competition, test_ids = preprocess(train_df, test_df)\n",
    "        \n",
    "        print(\"\\n--- Stage 1: Optimizing Hyperparameters with Optuna ---\")\n",
    "        \n",
    "        print(\"\\nOptimizing LightGBM...\")\n",
    "        lgbm_study = optuna.create_study(direction='minimize')\n",
    "        lgbm_study.optimize(lambda trial: optimize_lgbm(trial, X, y), n_trials=25) # Reduced trials for speed\n",
    "        best_lgb_params = lgbm_study.best_params\n",
    "        best_lgb_params['n_estimators'] = 2000\n",
    "        best_lgb_params['verbose'] = -1\n",
    "        best_lgb_params['n_jobs'] = -1\n",
    "        best_lgb_params['seed'] = 42\n",
    "        print(f\"Best LightGBM Params Found: {best_lgb_params}\")\n",
    "\n",
    "        print(\"\\nOptimizing XGBoost...\")\n",
    "        xgb_study = optuna.create_study(direction='minimize')\n",
    "        # MODIFICATION: Reduced n_trials for speed, since early stopping is disabled\n",
    "        xgb_study.optimize(lambda trial: optimize_xgb(trial, X, y), n_trials=15) \n",
    "        best_xgb_params = xgb_study.best_params\n",
    "        best_xgb_params['n_jobs'] = -1\n",
    "        best_xgb_params['seed'] = 42\n",
    "        print(f\"Best XGBoost Params Found: {best_xgb_params}\")\n",
    "        \n",
    "        stacking_and_predict(X, y, X_test_competition, test_ids, best_lgb_params, best_xgb_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_prices_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
