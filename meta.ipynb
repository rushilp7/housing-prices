{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-21 17:56:00,689] A new study created in memory with name: no-name-2b1445e6-4805-47bc-9935-12b388821e20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Starting improved preprocessing...\n",
      "\n",
      "--- Stage 1: Optimizing Hyperparameters for LightGBM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-21 17:56:01,970] Trial 0 finished with value: 0.1280662933317549 and parameters: {'learning_rate': 0.027921887246167586, 'lambda_l1': 0.03928927197758212, 'lambda_l2': 1.982061118526043, 'num_leaves': 31, 'feature_fraction': 0.7255741665678893, 'bagging_fraction': 0.736385356903716, 'min_child_samples': 29}. Best is trial 0 with value: 0.1280662933317549.\n",
      "[I 2025-09-21 17:56:03,721] Trial 1 finished with value: 0.13092726403757762 and parameters: {'learning_rate': 0.013204281154950922, 'lambda_l1': 0.904641021223619, 'lambda_l2': 14.729954629102162, 'num_leaves': 33, 'feature_fraction': 0.8755002799993742, 'bagging_fraction': 0.6037624154910486, 'min_child_samples': 34}. Best is trial 0 with value: 0.1280662933317549.\n",
      "[I 2025-09-21 17:56:05,847] Trial 2 finished with value: 0.13047906773794737 and parameters: {'learning_rate': 0.012885768120485919, 'lambda_l1': 3.2283815250373595, 'lambda_l2': 2.6899901376965025, 'num_leaves': 46, 'feature_fraction': 0.8568519661406544, 'bagging_fraction': 0.8180754672214874, 'min_child_samples': 24}. Best is trial 0 with value: 0.1280662933317549.\n",
      "[I 2025-09-21 17:56:08,978] Trial 3 finished with value: 0.1309411716481411 and parameters: {'learning_rate': 0.006666222841264334, 'lambda_l1': 0.43730502618904876, 'lambda_l2': 13.496481601912278, 'num_leaves': 39, 'feature_fraction': 0.8443893786464837, 'bagging_fraction': 0.5960121810906238, 'min_child_samples': 26}. Best is trial 0 with value: 0.1280662933317549.\n",
      "[I 2025-09-21 17:56:14,095] Trial 4 finished with value: 0.12351361868887767 and parameters: {'learning_rate': 0.017743378249939056, 'lambda_l1': 0.014210086356194328, 'lambda_l2': 0.04551277245867796, 'num_leaves': 47, 'feature_fraction': 0.5442289379185887, 'bagging_fraction': 0.743825902609032, 'min_child_samples': 17}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:16,411] Trial 5 finished with value: 0.12941260495622983 and parameters: {'learning_rate': 0.019357542043648916, 'lambda_l1': 0.09811671820181314, 'lambda_l2': 2.4528063472200605, 'num_leaves': 41, 'feature_fraction': 0.6213610221075919, 'bagging_fraction': 0.6788152125232274, 'min_child_samples': 14}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:18,633] Trial 6 finished with value: 0.13103638931316675 and parameters: {'learning_rate': 0.014989652150406464, 'lambda_l1': 0.15718229765635863, 'lambda_l2': 0.015518290342063312, 'num_leaves': 35, 'feature_fraction': 0.5495942835894401, 'bagging_fraction': 0.6000984192335661, 'min_child_samples': 35}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:19,897] Trial 7 finished with value: 0.13695976272903898 and parameters: {'learning_rate': 0.020611870418344325, 'lambda_l1': 1.959122725838059, 'lambda_l2': 0.01567121328967501, 'num_leaves': 37, 'feature_fraction': 0.7373401100744655, 'bagging_fraction': 0.5493526967758646, 'min_child_samples': 49}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:24,377] Trial 8 finished with value: 0.12768895168659916 and parameters: {'learning_rate': 0.0064698176281602305, 'lambda_l1': 0.2931657968466809, 'lambda_l2': 0.02796070305377706, 'num_leaves': 23, 'feature_fraction': 0.5114890567538201, 'bagging_fraction': 0.7259953544165535, 'min_child_samples': 28}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:25,100] Trial 9 finished with value: 0.13390721018160184 and parameters: {'learning_rate': 0.02635645984090972, 'lambda_l1': 15.74856211077532, 'lambda_l2': 0.14814005615055886, 'num_leaves': 40, 'feature_fraction': 0.8158969495529091, 'bagging_fraction': 0.748511148289656, 'min_child_samples': 40}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:28,514] Trial 10 finished with value: 0.12543233630758285 and parameters: {'learning_rate': 0.022957950691811228, 'lambda_l1': 0.012123569546149862, 'lambda_l2': 0.11564887727715137, 'num_leaves': 49, 'feature_fraction': 0.6354418466407233, 'bagging_fraction': 0.881722514552781, 'min_child_samples': 10}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:31,688] Trial 11 finished with value: 0.12603817011283946 and parameters: {'learning_rate': 0.02323066007510701, 'lambda_l1': 0.011674392004354444, 'lambda_l2': 0.1633297561089669, 'num_leaves': 50, 'feature_fraction': 0.6205262054334707, 'bagging_fraction': 0.8835279141823723, 'min_child_samples': 10}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:34,575] Trial 12 finished with value: 0.12457256311829054 and parameters: {'learning_rate': 0.02325825576621088, 'lambda_l1': 0.012007476948801313, 'lambda_l2': 0.08307237722672196, 'num_leaves': 49, 'feature_fraction': 0.6112608917917159, 'bagging_fraction': 0.8955503505369727, 'min_child_samples': 17}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:37,888] Trial 13 finished with value: 0.1268308484561249 and parameters: {'learning_rate': 0.01599669913635145, 'lambda_l1': 0.03713609567979764, 'lambda_l2': 0.05896184264725298, 'num_leaves': 45, 'feature_fraction': 0.5625374578094249, 'bagging_fraction': 0.8089400396006639, 'min_child_samples': 19}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:40,075] Trial 14 finished with value: 0.12522495995413438 and parameters: {'learning_rate': 0.02465628179078334, 'lambda_l1': 0.03998565051099286, 'lambda_l2': 0.4308584030656597, 'num_leaves': 46, 'feature_fraction': 0.6492526183842571, 'bagging_fraction': 0.8115837637436956, 'min_child_samples': 18}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:42,386] Trial 15 finished with value: 0.12700244265771812 and parameters: {'learning_rate': 0.01914607637582271, 'lambda_l1': 0.011149969649298165, 'lambda_l2': 0.043857799143445564, 'num_leaves': 28, 'feature_fraction': 0.5681747707688812, 'bagging_fraction': 0.6751578247044452, 'min_child_samples': 20}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:46,758] Trial 16 finished with value: 0.1255575272134212 and parameters: {'learning_rate': 0.011429756401322877, 'lambda_l1': 0.08071605343755131, 'lambda_l2': 0.5182152584738613, 'num_leaves': 43, 'feature_fraction': 0.6913429283564535, 'bagging_fraction': 0.8968510084968658, 'min_child_samples': 15}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:50,928] Trial 17 finished with value: 0.12558856854229355 and parameters: {'learning_rate': 0.02133374698407956, 'lambda_l1': 0.024431920718854867, 'lambda_l2': 0.4150162833478304, 'num_leaves': 50, 'feature_fraction': 0.508492196218606, 'bagging_fraction': 0.7727739628294016, 'min_child_samples': 22}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:52,928] Trial 18 finished with value: 0.12663821705172054 and parameters: {'learning_rate': 0.017720049833383128, 'lambda_l1': 0.17661063737684818, 'lambda_l2': 0.0860347502835524, 'num_leaves': 22, 'feature_fraction': 0.5809624487592826, 'bagging_fraction': 0.5029969362252167, 'min_child_samples': 15}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:54,293] Trial 19 finished with value: 0.1264456534304167 and parameters: {'learning_rate': 0.029894549039597165, 'lambda_l1': 0.02246556912957687, 'lambda_l2': 0.011520882744290632, 'num_leaves': 44, 'feature_fraction': 0.7853643186742761, 'bagging_fraction': 0.6460442567096731, 'min_child_samples': 34}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:56,981] Trial 20 finished with value: 0.13007478052992524 and parameters: {'learning_rate': 0.010696577375781977, 'lambda_l1': 0.061794832647849456, 'lambda_l2': 0.23933211770537324, 'num_leaves': 47, 'feature_fraction': 0.6785744980760786, 'bagging_fraction': 0.8475836868711281, 'min_child_samples': 45}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:56:58,658] Trial 21 finished with value: 0.1289941932648185 and parameters: {'learning_rate': 0.025155732856258134, 'lambda_l1': 0.02524589055709089, 'lambda_l2': 0.7700230243360975, 'num_leaves': 47, 'feature_fraction': 0.6523555431466119, 'bagging_fraction': 0.7881867289349387, 'min_child_samples': 17}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:00,521] Trial 22 finished with value: 0.12635075399716197 and parameters: {'learning_rate': 0.024337119248205776, 'lambda_l1': 0.010423858081892116, 'lambda_l2': 0.02848772388900036, 'num_leaves': 42, 'feature_fraction': 0.5952272889558977, 'bagging_fraction': 0.8425558628742505, 'min_child_samples': 21}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:03,193] Trial 23 finished with value: 0.12557296276415786 and parameters: {'learning_rate': 0.0272715533231639, 'lambda_l1': 0.05011257592529598, 'lambda_l2': 1.2169425657072515, 'num_leaves': 48, 'feature_fraction': 0.537073740012061, 'bagging_fraction': 0.8461388521305415, 'min_child_samples': 13}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:05,375] Trial 24 finished with value: 0.12485941848073745 and parameters: {'learning_rate': 0.021994299869723753, 'lambda_l1': 0.019816452749924163, 'lambda_l2': 0.31631696548476845, 'num_leaves': 44, 'feature_fraction': 0.6626842359003964, 'bagging_fraction': 0.7639972871514888, 'min_child_samples': 24}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:07,109] Trial 25 finished with value: 0.12827111664548646 and parameters: {'learning_rate': 0.022184691031178146, 'lambda_l1': 0.018238767513226426, 'lambda_l2': 0.2578412477815804, 'num_leaves': 44, 'feature_fraction': 0.5947695908919639, 'bagging_fraction': 0.7180844875413074, 'min_child_samples': 24}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:08,944] Trial 26 finished with value: 0.12821151481289936 and parameters: {'learning_rate': 0.01728439330502079, 'lambda_l1': 0.11932295563308948, 'lambda_l2': 0.060359848949660025, 'num_leaves': 38, 'feature_fraction': 0.7330892241672009, 'bagging_fraction': 0.7647962730502125, 'min_child_samples': 31}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:11,141] Trial 27 finished with value: 0.12772287799194945 and parameters: {'learning_rate': 0.019151860129771398, 'lambda_l1': 0.020168788481729174, 'lambda_l2': 0.0303609877923114, 'num_leaves': 50, 'feature_fraction': 0.6732532078150512, 'bagging_fraction': 0.6627870357942945, 'min_child_samples': 24}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:13,312] Trial 28 finished with value: 0.12850888740249827 and parameters: {'learning_rate': 0.020833278052345958, 'lambda_l1': 9.881935664331955, 'lambda_l2': 6.353864291569207, 'num_leaves': 43, 'feature_fraction': 0.5324583665793475, 'bagging_fraction': 0.7087164297945677, 'min_child_samples': 17}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:14,975] Trial 29 finished with value: 0.1282035960259315 and parameters: {'learning_rate': 0.028455652660369955, 'lambda_l1': 0.03641967150270702, 'lambda_l2': 0.08478681888608236, 'num_leaves': 29, 'feature_fraction': 0.7097411211875738, 'bagging_fraction': 0.7532869969927704, 'min_child_samples': 29}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:17,690] Trial 30 finished with value: 0.1272616477313738 and parameters: {'learning_rate': 0.016959949748835343, 'lambda_l1': 0.25520791608052934, 'lambda_l2': 0.22603944038949328, 'num_leaves': 36, 'feature_fraction': 0.6096648248823573, 'bagging_fraction': 0.6393080116429478, 'min_child_samples': 22}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:20,099] Trial 31 finished with value: 0.12590048949874996 and parameters: {'learning_rate': 0.025601894610213817, 'lambda_l1': 0.0368045460809395, 'lambda_l2': 0.5532555015857268, 'num_leaves': 46, 'feature_fraction': 0.6429343242333332, 'bagging_fraction': 0.8046774010865584, 'min_child_samples': 18}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:25,157] Trial 32 finished with value: 0.12528151567542173 and parameters: {'learning_rate': 0.024005391451885766, 'lambda_l1': 0.016856934208314918, 'lambda_l2': 0.31075456146922903, 'num_leaves': 48, 'feature_fraction': 0.6652994467556932, 'bagging_fraction': 0.8581407023009044, 'min_child_samples': 17}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:28,231] Trial 33 finished with value: 0.12578216636812228 and parameters: {'learning_rate': 0.02211444253442775, 'lambda_l1': 0.8403223666338756, 'lambda_l2': 1.3347263440186337, 'num_leaves': 46, 'feature_fraction': 0.7101236276723817, 'bagging_fraction': 0.8220564680492876, 'min_child_samples': 12}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:30,949] Trial 34 finished with value: 0.12723013194845872 and parameters: {'learning_rate': 0.014851860613141971, 'lambda_l1': 0.05446954053023437, 'lambda_l2': 1.0336826615108554, 'num_leaves': 41, 'feature_fraction': 0.7488213870040332, 'bagging_fraction': 0.7873261130455228, 'min_child_samples': 26}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:33,821] Trial 35 finished with value: 0.12845866622964214 and parameters: {'learning_rate': 0.01980883113273927, 'lambda_l1': 0.03229782551904696, 'lambda_l2': 4.063594264628188, 'num_leaves': 45, 'feature_fraction': 0.5914958825563797, 'bagging_fraction': 0.6969204624074202, 'min_child_samples': 26}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:37,475] Trial 36 finished with value: 0.12532904748563015 and parameters: {'learning_rate': 0.0268205556811599, 'lambda_l1': 0.01596432310249507, 'lambda_l2': 0.1113039448948492, 'num_leaves': 48, 'feature_fraction': 0.6338292541868226, 'bagging_fraction': 0.7447984228080453, 'min_child_samples': 20}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:39,565] Trial 37 finished with value: 0.12710654103681318 and parameters: {'learning_rate': 0.028469294872101193, 'lambda_l1': 0.08447477176278728, 'lambda_l2': 0.03842396955079024, 'num_leaves': 34, 'feature_fraction': 0.7645346413727288, 'bagging_fraction': 0.7840492507484155, 'min_child_samples': 15}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:41,969] Trial 38 finished with value: 0.12709358653752817 and parameters: {'learning_rate': 0.024312658713085595, 'lambda_l1': 0.6704135994850119, 'lambda_l2': 0.02025916376860289, 'num_leaves': 32, 'feature_fraction': 0.6585808951221002, 'bagging_fraction': 0.8204945024753383, 'min_child_samples': 23}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:43,913] Trial 39 finished with value: 0.127075348386593 and parameters: {'learning_rate': 0.018240171026438073, 'lambda_l1': 0.014921730996182726, 'lambda_l2': 0.15692247796300768, 'num_leaves': 40, 'feature_fraction': 0.6927352137471184, 'bagging_fraction': 0.8674321402190447, 'min_child_samples': 19}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:45,467] Trial 40 finished with value: 0.13084434180790241 and parameters: {'learning_rate': 0.014315737771399144, 'lambda_l1': 5.248068299822905, 'lambda_l2': 0.3865442099735214, 'num_leaves': 25, 'feature_fraction': 0.61247105876022, 'bagging_fraction': 0.7339252650565085, 'min_child_samples': 27}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:47,895] Trial 41 finished with value: 0.12593262465844823 and parameters: {'learning_rate': 0.02365693618953826, 'lambda_l1': 0.016376876699427157, 'lambda_l2': 0.335746043387583, 'num_leaves': 48, 'feature_fraction': 0.8903261660712496, 'bagging_fraction': 0.8634093060946015, 'min_child_samples': 17}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:50,444] Trial 42 finished with value: 0.12629317913733598 and parameters: {'learning_rate': 0.025421405708994732, 'lambda_l1': 0.02670999858448556, 'lambda_l2': 0.5903750075336884, 'num_leaves': 47, 'feature_fraction': 0.6570802841145252, 'bagging_fraction': 0.8343806844347086, 'min_child_samples': 13}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:53,382] Trial 43 finished with value: 0.12544847838658124 and parameters: {'learning_rate': 0.022104401406518537, 'lambda_l1': 1.750571525340894, 'lambda_l2': 1.7331461087812825, 'num_leaves': 49, 'feature_fraction': 0.6714922445107078, 'bagging_fraction': 0.8970515624025646, 'min_child_samples': 16}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:57,080] Trial 44 finished with value: 0.1249956380423202 and parameters: {'learning_rate': 0.02053760097094618, 'lambda_l1': 0.010254483131023015, 'lambda_l2': 19.997421802829166, 'num_leaves': 44, 'feature_fraction': 0.6257083801945462, 'bagging_fraction': 0.8581868028603935, 'min_child_samples': 11}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:57:59,716] Trial 45 finished with value: 0.12749111841767163 and parameters: {'learning_rate': 0.020547242294536965, 'lambda_l1': 0.010665901928516588, 'lambda_l2': 11.495371021082502, 'num_leaves': 42, 'feature_fraction': 0.5643910525645526, 'bagging_fraction': 0.8022376952481488, 'min_child_samples': 10}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:58:02,821] Trial 46 finished with value: 0.12798486955263452 and parameters: {'learning_rate': 0.015996136319119645, 'lambda_l1': 0.045868652822192366, 'lambda_l2': 3.478348958612919, 'num_leaves': 45, 'feature_fraction': 0.6300199369134938, 'bagging_fraction': 0.8750182719524823, 'min_child_samples': 12}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:58:05,112] Trial 47 finished with value: 0.12813889223075597 and parameters: {'learning_rate': 0.022792748440812513, 'lambda_l1': 0.010162968659162247, 'lambda_l2': 19.700820505947327, 'num_leaves': 38, 'feature_fraction': 0.5223329232585663, 'bagging_fraction': 0.8272430065027894, 'min_child_samples': 31}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:58:08,192] Trial 48 finished with value: 0.12832159451539282 and parameters: {'learning_rate': 0.0184664226600558, 'lambda_l1': 0.025707793415960873, 'lambda_l2': 0.06158592776269895, 'num_leaves': 44, 'feature_fraction': 0.5549184860400384, 'bagging_fraction': 0.7651491974556446, 'min_child_samples': 12}. Best is trial 4 with value: 0.12351361868887767.\n",
      "[I 2025-09-21 17:58:13,915] Trial 49 finished with value: 0.1258146718030398 and parameters: {'learning_rate': 0.00782282139530479, 'lambda_l1': 0.069673759315392, 'lambda_l2': 0.1879153061006512, 'num_leaves': 40, 'feature_fraction': 0.5795179559082763, 'bagging_fraction': 0.8807005791016468, 'min_child_samples': 14}. Best is trial 4 with value: 0.12351361868887767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM Params Found: {'learning_rate': 0.017743378249939056, 'lambda_l1': 0.014210086356194328, 'lambda_l2': 0.04551277245867796, 'num_leaves': 47, 'feature_fraction': 0.5442289379185887, 'bagging_fraction': 0.743825902609032, 'min_child_samples': 17, 'n_estimators': 3000, 'verbose': -1, 'n_jobs': -1, 'seed': 42}\n",
      "\n",
      "--- Starting Final Stacking Ensemble with More Folds ---\n",
      "--- Fold 1/15 ---\n",
      "--- Fold 2/15 ---\n",
      "--- Fold 3/15 ---\n",
      "--- Fold 4/15 ---\n",
      "--- Fold 5/15 ---\n",
      "--- Fold 6/15 ---\n",
      "--- Fold 7/15 ---\n",
      "--- Fold 8/15 ---\n",
      "--- Fold 9/15 ---\n",
      "--- Fold 10/15 ---\n",
      "--- Fold 11/15 ---\n",
      "--- Fold 12/15 ---\n",
      "--- Fold 13/15 ---\n",
      "--- Fold 14/15 ---\n",
      "--- Fold 15/15 ---\n",
      "\n",
      "--- Training Meta-Model ---\n",
      "Overall Stacked CV RMSE: 0.11462\n",
      "\n",
      "--- Submission Complete ---\n",
      "     Id      SalePrice\n",
      "0  1461  124099.107503\n",
      "1  1462  162556.675566\n",
      "2  1463  181032.675689\n",
      "3  1464  192295.431640\n",
      "4  1465  182103.821723\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import skew\n",
    "import optuna\n",
    "\n",
    "# --- (Data Loading and Preprocessing functions are unchanged) ---\n",
    "def load_data():\n",
    "    \"\"\"Loads the training and testing datasets.\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError:\n",
    "        return None, None\n",
    "\n",
    "def preprocess(train_df, test_df):\n",
    "    \"\"\"Handles missing values, feature engineering, and normalization.\"\"\"\n",
    "    print(\"Starting improved preprocessing...\")\n",
    "    test_ids = test_df['Id']\n",
    "    train_df = train_df.drop('Id', axis=1)\n",
    "    test_df = test_df.drop('Id', axis=1)\n",
    "\n",
    "    train_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\n",
    "    train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n",
    "    y = train_df['SalePrice']\n",
    "    all_data = pd.concat((train_df.drop('SalePrice', axis=1), test_df))\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType',\n",
    "                'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n",
    "                'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'):\n",
    "        all_data[col] = all_data[col].fillna('None')\n",
    "    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2',\n",
    "                'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "        all_data[col] = all_data[col].fillna(0)\n",
    "    for col in ('MSZoning', 'Utilities', 'Functional', 'Exterior1st', 'Exterior2nd',\n",
    "                'KitchenQual', 'SaleType', 'Electrical'):\n",
    "        all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "    all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # Feature Engineering\n",
    "    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "    all_data['Total_Bathrooms'] = (all_data['FullBath'] + 0.5 * all_data['HalfBath'] +\n",
    "                                   all_data['BsmtFullBath'] + 0.5 * all_data['BsmtHalfBath'])\n",
    "    all_data['YearBuilt_Age'] = all_data['YrSold'] - all_data['YearBuilt']\n",
    "    all_data['YearRemod_Age'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "    all_data['OverallQual_sq'] = all_data['OverallQual']**2\n",
    "    all_data['TotalSF_sq'] = all_data['TotalSF']**2\n",
    "    all_data['OverallQual_x_TotalSF'] = all_data['OverallQual'] * all_data['TotalSF']\n",
    "\n",
    "    # Skewness Transform\n",
    "    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.75].index\n",
    "    all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "    \n",
    "    all_data = pd.get_dummies(all_data)\n",
    "\n",
    "    X = all_data[:len(y)]\n",
    "    X_test_competition = all_data[len(y):]\n",
    "\n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\n",
    "    X_test_competition = pd.DataFrame(scaler.transform(X_test_competition), index=X_test_competition.index, columns=X_test_competition.columns)\n",
    "    \n",
    "    return X, y, X_test_competition, test_ids\n",
    "\n",
    "# --- 3. Hyperparameter Optimization (MODIFIED for more regularization) ---\n",
    "def optimize_lgbm(trial, X, y):\n",
    "    params = {\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.03),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.01, 20.0, log=True), # Increased lower bound\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.01, 20.0, log=True), # Increased lower bound\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 50), # Reduced upper bound\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 0.9),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 0.9),\n",
    "        'bagging_freq': 1,\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50), # Increased lower bound\n",
    "        'verbose': -1, 'n_jobs': -1, 'seed': 42\n",
    "    }\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    preds = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "# --- 4. Main Stacking and Submission Function (MODIFIED for more folds) ---\n",
    "def stacking_and_predict(X, y, X_test_competition, test_ids, lgb_params, xgb_params):\n",
    "    print(\"\\n--- Starting Final Stacking Ensemble with More Folds ---\")\n",
    "    kf = KFold(n_splits=15, shuffle=True, random_state=42) # Increased folds to 15\n",
    "\n",
    "    # Placeholders for predictions\n",
    "    oof_preds_lgb = np.zeros(X.shape[0])\n",
    "    oof_preds_xgb = np.zeros(X.shape[0])\n",
    "    test_preds_lgb = np.zeros(X_test_competition.shape[0])\n",
    "    test_preds_xgb = np.zeros(X_test_competition.shape[0])\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "        print(f\"--- Fold {fold+1}/15 ---\")\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        oof_preds_lgb[val_index] = lgb_model.predict(X_val)\n",
    "        test_preds_lgb += lgb_model.predict(X_test_competition) / kf.n_splits\n",
    "\n",
    "        # Use fixed, robust XGBoost params as it has environment issues\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror', eval_metric='rmse', n_estimators=2000,\n",
    "            learning_rate=0.01, max_depth=4, subsample=0.8,\n",
    "            colsample_bytree=0.7, reg_alpha=0.01, reg_lambda=0.1,\n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "        try:\n",
    "            xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "        except TypeError:\n",
    "            xgb_model.fit(X_train, y_train) # Fallback for your environment\n",
    "\n",
    "        oof_preds_xgb[val_index] = xgb_model.predict(X_val)\n",
    "        test_preds_xgb += xgb_model.predict(X_test_competition) / kf.n_splits\n",
    "\n",
    "    # Create meta-features\n",
    "    X_meta_train = pd.DataFrame({'lgb': oof_preds_lgb, 'xgb': oof_preds_xgb})\n",
    "    X_meta_test = pd.DataFrame({'lgb': test_preds_lgb, 'xgb': test_preds_xgb})\n",
    "\n",
    "    print(\"\\n--- Training Meta-Model ---\")\n",
    "    meta_model = ElasticNetCV(cv=5, random_state=42)\n",
    "    meta_model.fit(X_meta_train, y)\n",
    "    \n",
    "    oof_stacked_preds = meta_model.predict(X_meta_train)\n",
    "    stacked_rmse = np.sqrt(mean_squared_error(y, oof_stacked_preds))\n",
    "    print(f\"Overall Stacked CV RMSE: {stacked_rmse:.5f}\")\n",
    "\n",
    "    # Make final predictions\n",
    "    final_predictions_log = meta_model.predict(X_meta_test)\n",
    "    final_predictions = np.expm1(final_predictions_log)\n",
    "\n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({'Id': test_ids, 'SalePrice': final_predictions})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(\"\\n--- Submission Complete ---\")\n",
    "    print(submission.head())\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    train_df, test_df = load_data()\n",
    "    if train_df is not None and test_df is not None:\n",
    "        X, y, X_test_competition, test_ids = preprocess(train_df, test_df)\n",
    "        \n",
    "        # STAGE 1: Hyperparameter Optimization for LightGBM\n",
    "        print(\"\\n--- Stage 1: Optimizing Hyperparameters for LightGBM ---\")\n",
    "        lgbm_study = optuna.create_study(direction='minimize')\n",
    "        lgbm_study.optimize(lambda trial: optimize_lgbm(trial, X, y), n_trials=50)\n",
    "        best_lgb_params = lgbm_study.best_params\n",
    "        best_lgb_params['n_estimators'] = 3000 # Increase estimators for final run\n",
    "        best_lgb_params['verbose'] = -1\n",
    "        best_lgb_params['n_jobs'] = -1\n",
    "        best_lgb_params['seed'] = 42\n",
    "        print(f\"Best LightGBM Params Found: {best_lgb_params}\")\n",
    "\n",
    "        # STAGE 2: Run Stacking\n",
    "        # We use a fixed, robust XGBoost model due to environment issues and tune LGBM\n",
    "        stacking_and_predict(X, y, X_test_competition, test_ids, best_lgb_params, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_prices_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
